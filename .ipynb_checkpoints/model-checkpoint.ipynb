{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from pprint import pprint\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "space = re.compile(r'\\s')\n",
    "period = re.compile(r'(?<![A-Z])\\.|(?<!\\w)\\'(?!\\w)')\n",
    "punct = re.compile(r'[^\\'#@\\.\\w]')\n",
    "\n",
    "def tokenize(sent):\n",
    "    sent = space.split(sent)\n",
    "    to = []\n",
    "    tokens = []\n",
    "    for t in sent:\n",
    "        if t:\n",
    "            to += period.split(t)\n",
    "    for t in to:\n",
    "        if t:\n",
    "            tokens += punct.split(t)\n",
    "    return [a for a in tokens if a is not '']\n",
    "\n",
    "def get_lyrics_years(songs):\n",
    "    lyrics = []\n",
    "    years = []\n",
    "    \n",
    "    for song_id in list(songs.keys()):\n",
    "        lyrics.append(songs[song_id][\"lyrics\"].lower())\n",
    "        years.append(songs[song_id][\"year\"])\n",
    "        \n",
    "    return lyrics, years\n",
    "\n",
    "def dataset(lyrics, vocab_size):\n",
    "    # Words that were uncommon get noted as Out of bounds\n",
    "    count = [[\"OOB\", 0]]\n",
    "    count.extend(collections.Counter([word for lyric in lyrics for word in lyric]).most_common(vocab_size - 1))\n",
    "    word_to_index = {}\n",
    "    for word, _ in count:\n",
    "        word_to_index[word] = len(word_to_index)\n",
    "    encoded_lyrics = []\n",
    "    for song in lyrics:\n",
    "        encoded = []\n",
    "        for word in song:\n",
    "            index = word_to_index.get(word, 0)\n",
    "            if index == 0:\n",
    "                count[0][1] += 1\n",
    "            encoded.append(index)\n",
    "        encoded_lyrics.append(encoded)\n",
    "        index_to_word = dict(zip(word_to_index.values(), word_to_index.keys()))\n",
    "    return encoded_lyrics, count, word_to_index, index_to_word\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(encoded_lyrics[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizing lyrics]\n",
      "[Done]\n",
      "[Encoding Lyrics]\n",
      "[Done]\n",
      "17742\n"
     ]
    }
   ],
   "source": [
    "songs_filename = \"songs/songs.json\"\n",
    "songs_file = open(songs_filename, \"r+\")\n",
    "songs_dict = json.load(songs_file)\n",
    "\n",
    "\n",
    "lyrics, years = get_lyrics_years(songs_dict)\n",
    "\n",
    "tokenized_lyrics = []\n",
    "\n",
    "print(\"[Tokenizing lyrics]\")\n",
    "for l in lyrics:\n",
    "    tokenized_lyrics.append(tokenize(l))\n",
    "print(\"[Done]\")\n",
    "# Number of unique words to consider in our model\n",
    "vocabulary_size = 50000\n",
    "\n",
    "# encoded_lyrics is the original list of lyrics but with tokens\n",
    "# replaced with their corresponding dictionary index\n",
    "print(\"[Encoding Lyrics]\")\n",
    "encoded_lyrics, count, word_to_index, index_to_word = dataset(\n",
    "    tokenized_lyrics, vocabulary_size)\n",
    "print(\"[Done]\")\n",
    "\n",
    "del tokenized_lyrics\n",
    "del lyrics\n",
    "\n",
    "print(len(encoded_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('level', 836)\n"
     ]
    }
   ],
   "source": [
    "print(count[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
