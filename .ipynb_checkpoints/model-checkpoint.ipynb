{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from pprint import pprint\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "space = re.compile(r'\\s')\n",
    "period = re.compile(r'(?<![A-Z])\\.|(?<!\\w)\\'(?!\\w)')\n",
    "punct = re.compile(r'[^\\'#@\\.\\w]')\n",
    "\n",
    "def tokenize(sent):\n",
    "    sent = space.split(sent)\n",
    "    to = []\n",
    "    tokens = []\n",
    "    for t in sent:\n",
    "        if t:\n",
    "            to += period.split(t)\n",
    "    for t in to:\n",
    "        if t:\n",
    "            tokens += punct.split(t)\n",
    "    return [a for a in tokens if a is not '']\n",
    "\n",
    "def get_lyrics_years(songs):\n",
    "    lyrics = []\n",
    "    years = []\n",
    "    \n",
    "    for song_id in list(songs.keys()):\n",
    "        lyrics.append(songs[song_id][\"lyrics\"].lower())\n",
    "        years.append(songs[song_id][\"year\"])\n",
    "        \n",
    "    return lyrics, years\n",
    "\n",
    "def dataset(lyrics, vocab_size):\n",
    "    # Words that were uncommon get noted as Out of bounds\n",
    "    count = [[\"OOB\", 0]]\n",
    "    count.extend(collections.\n",
    "                 Counter([word for lyric in lyrics for word in lyric]).\n",
    "                 most_common(vocab_size - 1))\n",
    "    word_to_index = {}\n",
    "    for word, _ in count:\n",
    "        word_to_index[word] = len(word_to_index)\n",
    "    encoded_lyrics = []\n",
    "    for song in lyrics:\n",
    "        encoded = []\n",
    "        for word in song:\n",
    "            index = word_to_index.get(word, 0)\n",
    "            if index == 0:\n",
    "                count[0][1] += 1\n",
    "            encoded.append(index)\n",
    "        encoded_lyrics.append(encoded)\n",
    "        index_to_word = dict(zip(word_to_index.values(), word_to_index.keys()))\n",
    "    return encoded_lyrics, count, word_to_index, index_to_word\n",
    "\n",
    "def generate_batch(lyrics, batch_size, window_size):\n",
    "    batch = []\n",
    "    labels = []\n",
    "    \n",
    "    while len(batch) < batch_size:\n",
    "        # select random song\n",
    "        r_song_index = int(np.random.choice(len(lyrics), size=1))\n",
    "        r_song = lyrics[r_song_index]\n",
    "        # generate window\n",
    "        window = [r_song[max(i - window_size, 0):(i + window_size + 1)] for i, _ in enumerate(r_song)]\n",
    "        \n",
    "        batch_labels = [(r_song[i:i + window_size], r_song[i + window_size]) \n",
    "                        for i in range(len(r_song) - window_size)]\n",
    "        \n",
    "        # extract batch and label for this iteration\n",
    "        b, l = [list(x) for x in zip(*batch_labels)]\n",
    "        b = [x + [r_song_index] for x in b]\n",
    "        \n",
    "        batch.extend(b[:batch_size])\n",
    "        labels.extend(l[:batch_size])\n",
    "        \n",
    "    batch = batch[:batch_size]\n",
    "    labels = labels[:batch_size]\n",
    "    \n",
    "    batch = np.array(batch)\n",
    "    labels = np.transpose(np.array([labels]))\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "\n",
    "# Number of unique words to consider in our model\n",
    "vocabulary_size = 50000\n",
    "generations = 100000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# vector size\n",
    "embedding_size = 200\n",
    "song_embedding_size = 200\n",
    "concatenated_size = embedding_size + song_embedding_size\n",
    "\n",
    "# intervals to print out progress\n",
    "save_interval = 200\n",
    "print_loss_interval = 100\n",
    "\n",
    "# negative examples to sample\n",
    "num_sampled = 250\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "data_folder = \"model_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizing lyrics]\n",
      "[Done]\n",
      "[Encoding Lyrics]\n",
      "[Done]\n",
      "Number of songs: 17742\n"
     ]
    }
   ],
   "source": [
    "songs_filename = \"songs/songs.json\"\n",
    "songs_file = open(songs_filename, \"r+\")\n",
    "songs_dict = json.load(songs_file)\n",
    "\n",
    "\n",
    "lyrics, years = get_lyrics_years(songs_dict)\n",
    "\n",
    "tokenized_lyrics = []\n",
    "\n",
    "print(\"[Tokenizing lyrics]\")\n",
    "for l in lyrics:\n",
    "    tokenized_lyrics.append(tokenize(l))\n",
    "print(\"[Done]\")\n",
    "\n",
    "# encoded_lyrics is the original list of lyrics but with tokens\n",
    "# replaced with their corresponding dictionary index\n",
    "print(\"[Encoding Lyrics]\")\n",
    "encoded_lyrics, count, word_to_index, index_to_word = dataset(\n",
    "    tokenized_lyrics, vocabulary_size)\n",
    "print(\"[Done]\")\n",
    "\n",
    "del lyrics\n",
    "del tokenized_lyrics\n",
    "\n",
    "print(\"Number of songs:\", len(encoded_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Model]\n"
     ]
    }
   ],
   "source": [
    "print(\"[Creating Model]\")\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        x_inputs = tf.placeholder(tf.int32, shape=[None, window_size + 1])\n",
    "        y_target = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "    \n",
    "    with tf.name_scope('weights'):\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, concatenated_size],\n",
    "                               stddev=1.0 / np.sqrt(concatenated_size)))\n",
    "    with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            song_embeddings = tf.Variable(tf.random_uniform([len(encoded_lyrics), song_embedding_size], -1.0, 1.0))\n",
    "            embed = tf.zeros([batch_size, embedding_size])\n",
    "\n",
    "            # lookup word embeddings\n",
    "            for element in range(window_size):\n",
    "                embed += tf.nn.embedding_lookup(embeddings, x_inputs[:, element])\n",
    "\n",
    "            song_indices = tf.slice(x_inputs, [0, window_size], [batch_size, 1])\n",
    "            song_embed = tf.nn.embedding_lookup(song_embeddings, song_indices)\n",
    "\n",
    "            # concatenate embeddings\n",
    "            final_embed = tf.concat(axis=1, values=[embed, tf.squeeze(song_embed)])\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights, biases=nce_biases,\n",
    "                inputs=final_embed, labels=y_target,\n",
    "                num_sampled=num_sampled, \n",
    "                num_classes=vocabulary_size))\n",
    "    \n",
    "    # SGD optimizer\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=learning_rate).minimize(loss)\n",
    "            \n",
    "    # Create model saving operation\n",
    "    saver = tf.train.Saver({\"embeddings\": embeddings, \"song_embeddings\": song_embeddings})\n",
    "    \n",
    "    # Initialize global varialbles\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting Training]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_embeddings_interval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1790efdc2497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Save dictionary + embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msave_embeddings_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;31m# Save vocabulary dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_folder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'songs_vocab.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_embeddings_interval' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    print('[Starting Training]')\n",
    "    \n",
    "    loss_vec = []\n",
    "    loss_x_vec = []\n",
    "    \n",
    "    for i in range(generations):\n",
    "        batch_inputs, batch_labels = generate_batch(encoded_lyrics, batch_size,\n",
    "                                                   window_size)\n",
    "        \n",
    "        feed_dict = {x_inputs: batch_inputs, y_target: batch_labels}\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        # Return the loss\n",
    "        if (i + 1) % print_loss_interval == 0:\n",
    "            loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "            loss_vec.append(loss_val)\n",
    "            loss_x_vec.append(i + 1)\n",
    "            print('Loss at step {} : {}'.format(i + 1, loss_val))\n",
    "\n",
    "        # Save dictionary + embeddings\n",
    "        if (i + 1) % save_embeddings_interval == 0:\n",
    "            # Save vocabulary dictionary\n",
    "            with open(os.path.join(data_folder_name, 'songs_vocab.pkl'), 'wb') as f:\n",
    "                pickle.dump(word_to_index, f)\n",
    "\n",
    "            # Save embeddings\n",
    "            model_checkpoint_path = os.path.join(os.getcwd(), data_folder, 'doc2vec_song_embeddings.ckpt')\n",
    "            save_path = saver.save(sess, model_checkpoint_path)\n",
    "            print('Model saved in file: {}'.format(save_path))\n",
    "print(\"[Training doc2vec model Complete]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Start logistic model-------------------------\n",
    "max_words = 20\n",
    "logistic_batch_size = 500\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "# Need to keep the indices sorted to keep track of document index\n",
    "train_indices = np.sort(np.random.choice(len(years), round(0.8 * len(years)), replace=False))\n",
    "test_indices = np.sort(np.array(list(set(range(len(years))) - set(train_indices))))\n",
    "texts_train = [x for ix, x in enumerate(tokenized_lyrics) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(tokenized_lyrics) if ix in test_indices]\n",
    "target_train = np.array([x for ix, x in enumerate(years) if ix in train_indices])\n",
    "target_test = np.array([x for ix, x in enumerate(years) if ix in test_indices])\n",
    "\n",
    "# Convert texts to lists of indices\n",
    "text_data_train = np.array(text_helpers.text_to_numbers(texts_train, word_dictionary))\n",
    "text_data_test = np.array(text_helpers.text_to_numbers(texts_test, word_dictionary))\n",
    "\n",
    "# Pad/crop movie reviews to specific length\n",
    "text_data_train = np.array([x[0:max_words] for x in [y + [0] * max_words for y in text_data_train]])\n",
    "text_data_test = np.array([x[0:max_words] for x in [y + [0] * max_words for y in text_data_test]])\n",
    "\n",
    "# Define Logistic placeholders\n",
    "log_x_inputs = tf.placeholder(tf.int32, shape=[None, max_words + 1])  # plus 1 for doc index\n",
    "log_y_target = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "# Define logistic embedding lookup (needed if we have two different batch sizes)\n",
    "# Add together element embeddings in window:\n",
    "log_embed = tf.zeros([logistic_batch_size, embedding_size])\n",
    "for element in range(max_words):\n",
    "    log_embed += tf.nn.embedding_lookup(embeddings, log_x_inputs[:, element])\n",
    "\n",
    "log_doc_indices = tf.slice(log_x_inputs, [0, max_words], [logistic_batch_size, 1])\n",
    "log_doc_embed = tf.nn.embedding_lookup(doc_embeddings, log_doc_indices)\n",
    "\n",
    "# concatenate embeddings\n",
    "log_final_embed = tf.concat(1, [log_embed, tf.squeeze(log_doc_embed)])\n",
    "\n",
    "# Define model:\n",
    "# Create variables for logistic regression\n",
    "A = tf.Variable(tf.random_normal(shape=[concatenated_size, 1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1, 1]))\n",
    "\n",
    "# Declare logistic model (sigmoid in loss function)\n",
    "model_output = tf.add(tf.matmul(log_final_embed, A), b)\n",
    "\n",
    "# Declare loss function (Cross Entropy loss)\n",
    "logistic_loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(labels=model_output, logits=tf.cast(log_y_target, tf.float32)))\n",
    "\n",
    "# Actual Prediction\n",
    "prediction = tf.round(tf.sigmoid(model_output))\n",
    "predictions_correct = tf.cast(tf.equal(prediction, tf.cast(log_y_target, tf.float32)), tf.float32)\n",
    "accuracy = tf.reduce_mean(predictions_correct)\n",
    "\n",
    "# Declare optimizer\n",
    "logistic_opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "logistic_train_step = logistic_opt.minimize(logistic_loss, var_list=[A, b])\n",
    "\n",
    "# Intitialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Start Logistic Regression\n",
    "print('Starting Logistic Doc2Vec Model Training')\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "i_data = []\n",
    "for i in range(10000):\n",
    "    rand_index = np.random.choice(text_data_train.shape[0], size=logistic_batch_size)\n",
    "    rand_x = text_data_train[rand_index]\n",
    "    # Append review index at the end of text data\n",
    "    rand_x_doc_indices = train_indices[rand_index]\n",
    "    rand_x = np.hstack((rand_x, np.transpose([rand_x_doc_indices])))\n",
    "    rand_y = np.transpose([target_train[rand_index]])\n",
    "\n",
    "    feed_dict = {log_x_inputs: rand_x, log_y_target: rand_y}\n",
    "    sess.run(logistic_train_step, feed_dict=feed_dict)\n",
    "\n",
    "    # Only record loss and accuracy every 100 generations\n",
    "    if (i + 1) % 100 == 0:\n",
    "        rand_index_test = np.random.choice(text_data_test.shape[0], size=logistic_batch_size)\n",
    "        rand_x_test = text_data_test[rand_index_test]\n",
    "        # Append review index at the end of text data\n",
    "        rand_x_doc_indices_test = test_indices[rand_index_test]\n",
    "        rand_x_test = np.hstack((rand_x_test, np.transpose([rand_x_doc_indices_test])))\n",
    "        rand_y_test = np.transpose([target_test[rand_index_test]])\n",
    "\n",
    "        test_feed_dict = {log_x_inputs: rand_x_test, log_y_target: rand_y_test}\n",
    "\n",
    "        i_data.append(i + 1)\n",
    "\n",
    "        train_loss_temp = sess.run(logistic_loss, feed_dict=feed_dict)\n",
    "        train_loss.append(train_loss_temp)\n",
    "\n",
    "        test_loss_temp = sess.run(logistic_loss, feed_dict=test_feed_dict)\n",
    "        test_loss.append(test_loss_temp)\n",
    "\n",
    "        train_acc_temp = sess.run(accuracy, feed_dict=feed_dict)\n",
    "        train_acc.append(train_acc_temp)\n",
    "\n",
    "        test_acc_temp = sess.run(accuracy, feed_dict=test_feed_dict)\n",
    "        test_acc.append(test_acc_temp)\n",
    "    if (i + 1) % 500 == 0:\n",
    "        acc_and_loss = [i + 1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]\n",
    "        acc_and_loss = [np.round(x, 2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss (Test Loss): {:.2f} ({:.2f}). Train Acc (Test Acc): {:.2f} ({:.2f})'.format(\n",
    "            *acc_and_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count[1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
